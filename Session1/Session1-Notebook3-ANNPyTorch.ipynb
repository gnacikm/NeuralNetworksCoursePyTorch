{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tcdrr28Ij-D"
   },
   "source": [
    "# Neural Networks - ANNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-sHCChaJ6ml"
   },
   "source": [
    "# GPU in colab\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "*   Navigate to Editâ†’Notebook Settings\n",
    "*   select GPU from the Hardware Accelerator list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_GBIZNTKFfO"
   },
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6613,
     "status": "ok",
     "timestamp": 1737309063301,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "SudaPxJYw1_q",
    "outputId": "d2375958-149a-4343-d21c-e08c99da0903"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737309111593,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "SKvwjCkLHW4W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737309111804,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "oldiVlifxtl9",
    "outputId": "1fde448f-6b2a-46f2-ff38-315473ac0fc9"
   },
   "outputs": [],
   "source": [
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737309112040,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "TekOqiIdKNI7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXXsqqGZJ2aK"
   },
   "source": [
    "# Demonstration of the theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdQOPrDxWSfc"
   },
   "source": [
    "# Importing Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737309112439,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "h7NgF3mNHZ7M"
   },
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    batch_size: int, validation_split: float\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),  # Convert the images to PyTorch tensors\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create data loaders for the datasets\n",
    "\n",
    "    full_train_dataset: datasets.FashionMNIST = datasets.FashionMNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Calculate the sizes of each dataset\n",
    "    train_size: int = int((1 - validation_split) * len(full_train_dataset))\n",
    "    val_size: int = len(full_train_dataset) - train_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_train_dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader: DataLoader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader: DataLoader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    test_loader: DataLoader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6660,
     "status": "ok",
     "timestamp": 1737309119298,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "2KYQrFBKEzJ1",
    "outputId": "1e151713-bf54-4bee-cad8-7434b447db81"
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = load_data(batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737309119596,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "GGoklkHzWRKX"
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737309119596,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "WpjxVXNFH2Zq",
    "outputId": "b3a94edc-5648-4ecd-f33c-07d0c875016c"
   },
   "outputs": [],
   "source": [
    "print(train_features.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va6jeG60XEjz"
   },
   "source": [
    "Checking the number of samples and dimensionality of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dDwR7EiL4fu"
   },
   "source": [
    "The data represents images of\n",
    "\n",
    "0\tT-shirt/top\n",
    "\n",
    "1\tTrouser\n",
    "\n",
    "2\tPullover\n",
    "\n",
    "3\tDress\n",
    "\n",
    "4\tCoat\n",
    "\n",
    "5\tSandal\n",
    "\n",
    "6\tShirt\n",
    "\n",
    "7\tSneaker\n",
    "\n",
    "8\tBag\n",
    "\n",
    "9\tAnkle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C155BkEPXJ_i"
   },
   "source": [
    "Checking the labels in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737309119596,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "VCgVI4gfWmb2",
    "outputId": "9ae3c73f-d7b3-4097-f276-89cf80fb1cd4"
   },
   "outputs": [],
   "source": [
    "unique_labels = train_labels.unique(sorted=True)\n",
    "numb_labels = unique_labels.size()  # total number of labels\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737309119596,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "gtNg4qWSI4TX",
    "outputId": "24f72d4e-45ac-4a06-ecf8-12c39cc90e3e"
   },
   "outputs": [],
   "source": [
    "numb_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKD7OJE4axeP"
   },
   "source": [
    "Plotting images in different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737309119596,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "ZWzzZUUQxY8G"
   },
   "outputs": [],
   "source": [
    "def plot_unique_labels_with_categories(\n",
    "    train_features: torch.Tensor,\n",
    "    train_labels: torch.Tensor,\n",
    "    unique_labels: list[int],\n",
    "    label_to_category: dict[int, str],\n",
    "    nrows: int = 5,\n",
    "    ncols: int = 2,\n",
    "    figsize: tuple = (10, 15),\n",
    ") -> None:\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    ax = ax.flatten()  # Flatten the axes array for easier indexing\n",
    "\n",
    "    # Map unique labels to their corresponding image indices\n",
    "    label_indices = {\n",
    "        label.item(): (train_labels == label).nonzero(as_tuple=True)[0][0].item()\n",
    "        for label in unique_labels\n",
    "        if (train_labels == label).any()\n",
    "    }\n",
    "\n",
    "    for index, label in enumerate(unique_labels):\n",
    "        if index >= len(ax):  # Prevent index errors if more labels than axes\n",
    "            break\n",
    "        if label.item() not in label_indices:  # Skip missing labels\n",
    "            continue\n",
    "        img_index = label_indices[label.item()]\n",
    "        ax[index].imshow(train_features[img_index, 0, :, :].cpu(), cmap=\"gray\")\n",
    "        ax[index].set_title(f\"{label_to_category[label.item()]}\")\n",
    "        ax[index].axis(\"off\")\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for index in range(len(unique_labels), len(ax)):\n",
    "        ax[index].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1737309120247,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "OwnS5M9exzgs",
    "outputId": "24e3d9a7-3bf2-4e7a-d4d2-df9bbb91d414"
   },
   "outputs": [],
   "source": [
    "# Define the mapping of labels to category names\n",
    "label_to_category = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\",\n",
    "}\n",
    "\n",
    "\n",
    "# Plot with category names\n",
    "plot_unique_labels_with_categories(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    unique_labels,\n",
    "    label_to_category,\n",
    "    nrows=5,\n",
    "    ncols=2,\n",
    "    figsize=(5, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJdB0kNPXJeZ"
   },
   "source": [
    "## Tensors - multidimensional arrays\n",
    "\n",
    "Our images are grayscale, they are matrices so that each entry of a matrix is a pixel whose number represents the intensity of the colour,\n",
    "- 0 represents black and\n",
    "- 255 represents white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWOcLTw0csFD"
   },
   "source": [
    "We can think that grayscale images have only one colour channel, what about the colour images?\n",
    "\n",
    "Colour may be important for a classification task.\n",
    "\n",
    "Colour images are represented as 3 matrices,\n",
    "- the first matrix is for RED channel, 255 represents RED and 0 BLACK;\n",
    "- the second matrix is for GREEN channel, 255 represents GREEN and 0 BLACK;\n",
    "- the third matrix is for the BLUE channel, 255 represents BLUE and 0 BLACK;\n",
    "\n",
    "Rather than having 3 different matrices, we combain them into one array, we call this multidimensional array a tensor.\n",
    "\n",
    "Given a colour image of size $p \\times q$ the shape of the tensor will be\n",
    "\n",
    "`` (p, q, 3) `` where 3 indicates that there are 3 colour channels.\n",
    "\n",
    "Having $N$ colour images we will store tham in a tensor of the shape:\n",
    "\n",
    "`` (N, p, q, 3)``\n",
    "\n",
    "To get used to tensor structure let's convert our digits to tensor with 1 channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oJ6Mx-tejcB"
   },
   "source": [
    "## Preprocessing an image\n",
    "\n",
    "Later we will learn how to properly pre-process images. Now we will do a small tweak, as neural networks work better with numbers between 0 and 1, let's scale our images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y06U37uRfO2z"
   },
   "source": [
    "# Building a feedforward neural network for MNIST\n",
    "\n",
    "For our neural networks we will set up a model as `nn.Module` sub-class.\n",
    "\n",
    "Let's set up our model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giPkOGb8ft57"
   },
   "source": [
    "**Wait a second** We studied the neural networks so that their input was a vector, not a matrix or tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5o35HBCf6in"
   },
   "source": [
    "To handle that we first add the FLATTENING layer, that will flatten the input so that it is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZQNdiiQgGCC"
   },
   "source": [
    "Now we add a couple of hidden-layers, these are called Linear layers in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1737310009038,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "SzHsd3aogE0p"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_labels: int):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_one = nn.Linear(28 * 28, 32)\n",
    "        self.dense_two = nn.Linear(32, 16)\n",
    "        self.dense_three = nn.Linear(16, num_labels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        x_tensor = self.flatten(x_tensor)\n",
    "        x_tensor = self.relu(self.dense_one(x_tensor))\n",
    "        x_tensor = self.relu(self.dense_two(x_tensor))\n",
    "        x_tensor = self.dense_three(x_tensor)\n",
    "        return x_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1GfPNmuruX6"
   },
   "source": [
    "## Sequential API\n",
    "\n",
    "Model can be prepared with sequential api, very similar the one knows from Keras (with TensorFlow backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1737310071561,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "ZRVXiKO4rtxd"
   },
   "outputs": [],
   "source": [
    "num_labels: int = 10\n",
    "\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Flatten(1, -1),  # Flatten the input\n",
    "    nn.Linear(28 * 28, 32),  # Dense layer 1\n",
    "    nn.ReLU(),  # Activation after dense layer 1\n",
    "    nn.Linear(32, 16),  # Dense layer 2\n",
    "    nn.ReLU(),  # Activation after dense layer 2\n",
    "    nn.Linear(16, num_labels),  # Dense layer 3 (output layer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf6gtMWEgWFa"
   },
   "source": [
    "Note that we choose an activation functino for this layers to be ReLU, remember $ReLU(x) = \\max\\{0, x\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKYkxp0xggeb"
   },
   "source": [
    "Now we will add the OUTPUT LAYER, it need to have the right number of nodes in the output to match the number of labels in Fashion MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NSO8BKogxkm"
   },
   "source": [
    "We equipped the output layer with raw logits instead of the ``softmax`` activation function. This is because the loss function we use, ``CrossEntropyLoss``, internally applies ``logsoftmax`` to the logits. This approach ensures numerical stability and correct probability computations.\n",
    "\n",
    "``LogSoftmax`` is a combination of the ``log`` and ``softmax`` functions, and it transforms the output vector into a log-probability distribution.\n",
    "\n",
    "For a vector $z = (z_1, \\ldots, z_n) \\in \\mathbb{R}^n$, the value of the softmax function $\\sigma(z) = (\\sigma(z)_1, \\sigma(z)_2, \\ldots, \\sigma(z)_n)$ is given by:\n",
    "$$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$\n",
    "\n",
    "If we wish to define ``logsoftmax`` by hand, it is given by:\n",
    "$$\\log\\sigma(z)_i = z_i - \\log\\left(\\sum_{j=1}^n e^{z_j}\\right)$$\n",
    "\n",
    "This transformation ensures that each entry of the output vector is a valid log-probability. The highest log-probability cell corresponds to the digit prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1737309949891,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "oDHSAamKNnW-",
    "outputId": "48841407-8eb4-49ae-aeb7-7596e9f72b32"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    output = np.exp(x) / np.sum(np.exp(x))\n",
    "    return output\n",
    "\n",
    "\n",
    "def logsoftmax(x):\n",
    "    return x - np.log(np.sum(np.exp(x)))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 1.0, 3.0])\n",
    "print(softmax(x))\n",
    "print(logsoftmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERAi9Vinhgnh"
   },
   "source": [
    "### Selecting the LOSS function, the metrics for ACCURACY and the OPTIMIZER.\n",
    "\n",
    "For classification problems we will pick **categorical cross-entropy** loss  function.\n",
    "\n",
    "As score metrics we will pick **accuracy**.\n",
    "\n",
    "For an optimizer let's use **STOCHASTIC GRADIENT DESCENT** (SGD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1737309983827,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "FljaxyNltgiW"
   },
   "outputs": [],
   "source": [
    "num_labels: int = 10\n",
    "model: Model = Model(num_labels)\n",
    "\n",
    "criterion: nn.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer: optim.SGD = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737309950677,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "2UT7dVVFsrJB",
    "outputId": "cbe4ba97-1a33-42f0-816d-6eeec1d7e25e"
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1737309951359,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "bYWJ7712ssqK",
    "outputId": "fd365fca-b4f6-4f63-d923-c349440856df"
   },
   "outputs": [],
   "source": [
    "summary(model_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jzO51RSkaZL"
   },
   "source": [
    "# Training the Network\n",
    "\n",
    "Let's select a number of epochs, let's set it for 5 to beging with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1737309952978,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "J03oLyZLkXVF"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737309953690,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "CnOenQE10ij1"
   },
   "outputs": [],
   "source": [
    "# Define a function to train for one epoch\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss: float = 0.0\n",
    "    total_samples: int = 0\n",
    "    correct_predictions: int = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs: torch.Tensor = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss: torch.Tensor = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted_labels == targets).sum().item()\n",
    "\n",
    "        # Update running loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss: float = total_loss / len(train_loader)\n",
    "    accuracy: float = (correct_predictions / total_samples) * 100\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737309954313,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "ThQ9Q6vZ0qBZ"
   },
   "outputs": [],
   "source": [
    "# Define a function to evaluate the model on the validation set\n",
    "def evaluate_model(\n",
    "    model: nn.Module, val_loader: DataLoader, criterion: nn.Module, device: torch.device\n",
    ") -> tuple[float, float, list[int], list[int]]:\n",
    "    model.eval()\n",
    "    total_loss: float = 0.0\n",
    "    total_samples: int = 0\n",
    "    correct_predictions: int = 0\n",
    "    y_true: list[int] = []\n",
    "    y_pred: list[int] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs: torch.Tensor = model(inputs)\n",
    "            loss: torch.Tensor = criterion(outputs, targets)\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, dim=1)\n",
    "            y_true.extend(targets.cpu().tolist())\n",
    "            y_pred.extend(predicted_labels.cpu().tolist())\n",
    "\n",
    "            total_samples += targets.size(0)\n",
    "            correct_predictions += (predicted_labels == targets).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss: float = total_loss / len(val_loader)\n",
    "    accuracy: float = (correct_predictions / total_samples) * 100\n",
    "    return average_loss, accuracy, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737309954815,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "tHkRfDjI0qu-"
   },
   "outputs": [],
   "source": [
    "# Define a function to train the model for multiple epochs\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    ") -> tuple[list[float], list[float], list[float], list[float]]:\n",
    "    training_loss_history: list[float] = []\n",
    "    training_accuracy_history: list[float] = []\n",
    "    validation_loss_history: list[float] = []\n",
    "    validation_accuracy_history: list[float] = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        val_loss, val_accuracy, _, _ = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        training_loss_history.append(train_loss)\n",
    "        training_accuracy_history.append(train_accuracy)\n",
    "        validation_loss_history.append(val_loss)\n",
    "        validation_accuracy_history.append(val_accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, \"\n",
    "            f\"Val Accuracy: {val_accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        training_loss_history,\n",
    "        training_accuracy_history,\n",
    "        validation_loss_history,\n",
    "        validation_accuracy_history,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRA-zdhCkm34"
   },
   "source": [
    "Now we will fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1737310294114,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "l0uJszC5054e"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    training_loss_history,\n",
    "    training_accuracy_history,\n",
    "    validation_loss_history,\n",
    "    validation_accuracy_history,\n",
    ") = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27UCT2zLlPZu"
   },
   "source": [
    "# Model summary\n",
    "\n",
    "We can display the model structure, it will also show us how many parameters (weights, biases) there is for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737300719322,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "EmBW2KTekxcS",
    "outputId": "f69ee96b-cb06-4aa8-a341-61ccc84d7492"
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwUlKLFGliGT"
   },
   "source": [
    "# Plotting TRAINING AND VALIDATION curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1737300719827,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "XoaJ_3iSlp08",
    "outputId": "ba91db1c-2c3b-4686-f4f9-82fbe7a9add5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_loss_history, label=\"training loss\")\n",
    "plt.plot(validation_loss_history, label=\"val loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracy_history, label=\"training acc\")\n",
    "plt.plot(validation_accuracy_history, label=\"val acc\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X55QVk6eqgtX"
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7212,
     "status": "ok",
     "timestamp": 1737300727035,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "O22rf2cG3GGd",
    "outputId": "0d39ef61-42b1-40b4-8092-1c5c57a2adcd"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, y_pred, y_true = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx2hPD-wrApU"
   },
   "source": [
    "# Confusion matrix\n",
    "\n",
    "We produce the confusion matrix of our model.\n",
    "\n",
    "A **confusion matrix** contains a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1lzHvXWrDzM"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw6jgeh8LH7S"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true: list[int], y_pred: list[int]) -> np.ndarray:\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    index_conf = np.arange(conf_matrix.shape[0])\n",
    "    df_cm = pd.DataFrame(conf_matrix, index_conf, index_conf)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sn.set(font_scale=1.4)\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt=\"g\")\n",
    "    plt.show()\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1737300730202,
     "user": {
      "displayName": "Michal Gnacik",
      "userId": "05693853346063217104"
     },
     "user_tz": 0
    },
    "id": "l16pVHelrjwa",
    "outputId": "4c7d3284-bbb2-45d8-efa8-4fd2b6dd93d0"
   },
   "outputs": [],
   "source": [
    "conf_matrix = plot_confusion_matrix(y_true, y_pred)  # confusion matrix\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AcmB3Cj4sfo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
